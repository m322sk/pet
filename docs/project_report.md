# Пет-проект: аналитика кэшбэка Tinkoff Black

## 0. Входные данные

**Источник**: `tinkoff2_cashback.csv` + описание полей из `T_cashback_Описание_датасета.pdf`.

Ключевые особенности датасета:
- Месячная агрегация (апрель–сентябрь), в строке — клиент и месяц покупок.
- Для каждой категории есть три группы полей: **оборот**, **активация кэшбэка** и **полученный кэшбэк**.
- В полях активации встречаются пропуски (категория недоступна), 0 (доступна, но не выбрана), 1 (доступна и выбрана).

Эта специфика важна для очистки и интерпретации данных: значения `NULL` нельзя смешивать с `0` — это разные состояния выбора категории.

## 1. Работа с данными (SQL + немного Python)

### 1.1. Подготовка схемы
- **Staging-таблица** хранит сырые значения строками, чтобы не терять пропуски и избежать ошибок парсинга.
- **Нормализованная модель**:
  - `dim_clients` — уникальные клиенты.
  - `dim_months` — справочник месяцев.
  - `fact_cashback` — фактические обороты/активации/кэшбэк по клиенту и месяцу.

SQL находится в `sql/01_schema.sql`.

### 1.2. Очистка
Цели очистки:
- привести типы (даты, целые, float),
- корректно отличить `NULL` от `0` в активациях,
- убрать дубликаты по ключу `(client_id, month)`

SQL находится в `sql/02_cleaning.sql`.

### 1.3. Python-скрипт для выгрузки/проверок
`scripts/profile_data.py`:
- читает CSV стандартным модулем `csv`,
- считает базовые показатели качества (доля пропусков, дубликаты ключей),
- сохраняет сводку в `docs/data_quality_report.md`.

## 2. Дашборды (ClickHouse)

Поскольку данные агрегированы по месяцам, я использую **MAU** вместо DAU/WAU.
Ключевые метрики:
- MAU по месяцам (активные клиенты с оборотом > 0)
- средний оборот по клиенту
- доля клиентов, активировавших кэшбэк в категории
- распределение оборотов по категориям

SQL для виджетов и графиков — `sql/03_metrics.sql`.

## 3. A/B тест (SQL + немного Python)

**Гипотеза**: клиенты, получившие категории с повышенным кэшбэком, увеличат оборот в этих категориях.

### 3.1. План эксперимента
- Сплит по `cityHash64(client_id)` для стабильного разбиения.
- Контроль/тест с равными долями.
- Метрика: средний оборот в «активированных» категориях.

### 3.2. Подготовка данных (SQL)
- Расчет пользовательских метрик по месяцу.
- Выгрузка в CSV для t-теста.

SQL — `sql/04_ab_test.sql`.

### 3.3. Статтест (Python)
`scripts/ab_test.py`:
- принимает CSV с метрикой,
- считает t-статистику и p-value (нормальная аппроксимация),
- выводит интерпретацию.

## 4. Пайплайн (концепт)

Цель — показать, как выстроить поток данных:
1. **Источник**: CSV/операционная БД.
2. **Загрузка**: staging в ClickHouse.
3. **Transform**: SQL-процедуры очистки.
4. **BI**: Superset/Redash.

Подробности в `docs/pipeline_design.md`.

## 5. Система алертов

- ежедневные алерты по провалам MAU и оборота,
- правило «3σ» для поиска аномалий,
- отправка в Telegram через Airflow (описание и пример SQL).

SQL и правила — `sql/05_alerts.sql`, описание — `docs/alerts.md`.

---

## Как запустить

1. Создать таблицы:
```sql
-- ClickHouse
source sql/01_schema.sql
```
2. Загрузить CSV в staging:
```bash
clickhouse-client --query="INSERT INTO cashback_staging FORMAT CSV" < tinkoff2_cashback.csv
```
3. Очистить и наполнить модель:
```sql
source sql/02_cleaning.sql
```
4. Метрики и дашборды — `sql/03_metrics.sql`.
5. A/B тест — `sql/04_ab_test.sql` + `python scripts/ab_test.py --input ab_export.csv`.
