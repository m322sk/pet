# Пет-проект: аналитика кэшбэка Tinkoff Black

## 0. Входные данные

**Источник**: `tinkoff2_cashback.csv` + описание полей из `T_cashback_Описание_датасета.pdf`.

Ключевые особенности датасета:
- Месячная агрегация (апрель–сентябрь), в строке — клиент и месяц покупок.
- Для каждой категории есть три группы полей: **оборот**, **активация кэшбэка** и **полученный кэшбэк**.
- В полях активации встречаются пропуски (категория недоступна), 0 (доступна, но не выбрана), 1 (доступна и выбрана).

Эта специфика важна для очистки и интерпретации данных: значения `NULL` нельзя смешивать с `0` — это разные состояния выбора категории.

## 1. Работа с данными (SQL + немного Python)

### 1.1. Подготовка схемы
- **Staging-таблица** хранит сырые значения строками, чтобы не терять пропуски и избежать ошибок парсинга.
- **Нормализованная модель**:
  - `dim_clients` — уникальные клиенты.
  - `dim_months` — справочник месяцев.
  - `fact_cashback_category` — нормализованный факт (client_id, month, category).
  - `mart_client_month` — витрина клиент-месяц с агрегатами и QC-метриками.

PostgreSQL-скрипт схемы находится в `sql/postgres_01_schema.sql`.

### 1.2. Очистка
Цели очистки:
- привести типы (даты, целые, float),
- корректно отличить `NULL` от `0` в активациях,
- убрать дубликаты по ключу `(client_id, month)`
- закодировать состояние выбора как `activation_state` (`not_offered`, `offered_not_chosen`, `chosen`)

SQL очистки для PostgreSQL находится в `sql/postgres_02_cleaning.sql`.
Там же формируется QC-проверка: `eligible_cnt` должен быть 7 или 8, и строится
`has_subscription_guess` на основе количества доступных категорий.

### 1.3. Python-скрипт для выгрузки/проверок
`scripts/profile_data.py`:
- читает CSV стандартным модулем `csv`,
- считает базовые показатели качества (доля пропусков, дубликаты ключей),
- сохраняет сводку в `docs/data_quality_report.md`.

## 2. Дашборды (PostgreSQL как витрина)

Поскольку данные агрегированы по месяцам, я использую **MAU** вместо DAU/WAU.
Ключевые метрики:
- MAU по месяцам (активные клиенты с оборотом > 0)
- средний оборот по клиенту
- доля клиентов, активировавших кэшбэк в категории
- распределение оборотов по категориям

SQL для витрины в PostgreSQL — `sql/postgres_03_metrics.sql`.

## 3. Анализ эффекта (observational / quasi-experiment)

**Гипотеза**: клиенты, получившие категории с повышенным кэшбэком, увеличат оборот в этих категориях.

### 3.1. Важный дисклеймер
- В данных нет реального рандомизированного эксперимента.
- Категории с повышенным кэшбэком подбирались не случайно (есть подписка и 7/8 категорий).
- Поэтому причинные выводы делать нельзя — это **observational analysis**.

### 3.2. Подходы анализа
- **Within-user**: сравнение оборота в выбранной категории до/после выбора.
- **Нормировка** на общий оборот месяца, чтобы контролировать базовую активность.

SQL для PostgreSQL — `sql/postgres_04_ab_test.sql`.

### 3.3. Про t-test
`scripts/ab_test.py` оставлен как учебный пример техники, но в этом датасете
он не доказывает причинность. Для отчета я использую observational-подходы.

## 4. Пайплайн (PostgreSQL, batch/cron)

Цель — показать, как выстроить поток данных:
1. **Источник**: CSV загружается в PostgreSQL (основная БД).
2. **Transform**: очистка в PostgreSQL (`sql/postgres_02_cleaning.sql`).
3. **Materialized views / mart_***: витрины прямо в PostgreSQL (без отдельной БД).
4. **BI**: Superset/Redash подключаются к PostgreSQL.

Подробности в `docs/pipeline_design.md`.

## 5. Система алертов

- ежедневные алерты по провалам MAU и оборота,
- правило «3σ» для поиска аномалий,
- отправка в Telegram через cron/Airflow (описание и пример SQL).

SQL и правила для PostgreSQL — `sql/postgres_05_alerts.sql`, описание — `docs/alerts.md`.


